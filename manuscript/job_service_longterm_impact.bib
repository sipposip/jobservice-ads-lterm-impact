
@article{allhutterAlgorithmicProfilingJob2020,
  title = {Algorithmic {{Profiling}} of {{Job Seekers}} in {{Austria}}: {{How Austerity Politics Are Made Effective}}},
  shorttitle = {Algorithmic {{Profiling}} of {{Job Seekers}} in {{Austria}}},
  author = {Allhutter, Doris and Cech, Florian and Fischer, Fabian and Grill, Gabriel and Mager, Astrid},
  year = {2020},
  volume = {3},
  publisher = {{Frontiers}},
  issn = {2624-909X},
  doi = {10.3389/fdata.2020.00005},
  abstract = {As of 2020, the Public Employment Service Austria (AMS) makes use of algorithmic profiling of job seekers to increase the efficiency of its counselling process and the effectivity of active labor market programs. Based on a statistical model of job seekers' prospects on the labor market, the system - that has become known as the AMS algorithm - is designed to classify clients of the AMS into three categories: those with high chances to find a job within half a year, those with mediocre prospects on the job market, and those clients with a bad outlook of employment in the next two years. Depending on the category a particular job seeker is classified under they will be offered differing support in (re)entering the labor market. Based in science and technology studies, critical data studies and research on fairness, accountability and transparency of algorithmic systems, this paper examines the inherent politics of the AMS algorithm. An in-depth analysis of relevant technical documentation and policy documents investigates crucial conceptual, technical and social implications of the system. The analysis shows how the design of the algorithm is influenced by technical affordances, but also by social values, norms, and goals. A discussion of the tensions, challenges and possible biases that the system entails calls into question the objectivity and neutrality of data claims and of high hopes pinned on evidence-based decision-making. In this way, the paper sheds light on the co-production of (semi)automated managerial practices in employment agencies and the framing of unemployment under austerity politics.},
  file = {/home/sscher/Zotero/storage/395M3WTA/Allhutter et al. - 2020 - Algorithmic Profiling of Job Seekers in Austria H.pdf},
  journal = {Frontiers in Big Data},
  keywords = {Algorithmic Profiling,Austerity politics,Austria,big data,Co-production,critical data studies,Public employment service,qualitative research},
  language = {English}
}

@inproceedings{damourFairnessNotStatic2020,
  title = {Fairness Is Not Static: Deeper Understanding of Long Term Fairness via Simulation Studies},
  shorttitle = {Fairness Is Not Static},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {D'Amour, Alexander and Srinivasan, Hansa and Atwood, James and Baljekar, Pallavi and Sculley, D. and Halpern, Yoni},
  year = {2020},
  month = jan,
  pages = {525--534},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3351095.3372878},
  abstract = {As machine learning becomes increasingly incorporated within high impact decision ecosystems, there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences. Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics. This is understandable; long term dynamics are hard to assess, particularly because they do not align with the traditional supervised ML research framework that uses fixed data sets. To address this structural difficulty in the field, we advocate for the use of simulation as a key tool in studying the fairness of algorithms. We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans, college admissions, and allocation of attention. By analyzing how learning agents interact with these systems in simulation, we are able to extend previous work, showing that static or single-step analyses do not give a complete picture of the long-term consequences of an ML-based decision system. We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research, available at https://github.com/google/ml-fairness-gym.},
  file = {/home/sscher/Zotero/storage/S5MFRIVD/D'Amour et al. - 2020 - Fairness is not static deeper understanding of lo.pdf},
  isbn = {978-1-4503-6936-7},
  series = {{{FAT}}* '20}
}

@article{ensignRunawayFeedbackLoops,
  title = {Runaway {{Feedback Loops}} in {{Predictive Policing}}},
  author = {Ensign, Danielle and Friedler, Sorelle A and Neville, Scott and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  pages = {12},
  abstract = {Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been empirically shown to be susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.},
  file = {/home/sscher/Zotero/storage/DNEC5ATN/Ensign et al. - Runaway Feedback Loops in Predictive Policing.pdf},
  language = {en}
}

@article{hollAMSArbeitsmarktChancenModell,
  title = {{Das AMS-Arbeitsmarkt- chancen-Modell}},
  author = {Holl, J{\"u}rgen and Kernbei{\ss}, G{\"u}nter and {Wagner-Pinter}, Michael},
  pages = {16},
  file = {/home/sscher/Zotero/storage/R6JXR5IM/Holl et al. - Das AMS-Arbeitsmarkt- chancen-Modell.pdf},
  language = {de}
}

@incollection{holzleithnerEUrechtlicheBestimmungenDiskriminierungsverbot2017,
  title = {{EU-rechtliche Bestimmungen zum Diskriminierungsverbot}},
  booktitle = {{Handbuch Diskriminierung}},
  author = {Holzleithner, Elisabeth},
  editor = {Scherr, Albert and {El-Mafaalani}, Aladin and Y{\"u}ksel, G{\"o}k{\c c}en},
  year = {2017},
  pages = {211--237},
  publisher = {{Springer Fachmedien}},
  address = {{Wiesbaden}},
  doi = {10.1007/978-3-658-10976-9_12},
  abstract = {Die Prinzipien der Gleichheit und der Nichtdiskriminierung haben in der Europ\"aischen Union eine herausragende Bedeutung. Aktivit\"aten des EU-Gesetzgebers haben bereits zu einer Mehrzahl von Richtlinien gef\"uhrt, die eine Bek\"ampfung von Diskriminierungen erm\"oglichen und weitergehende Ma\ss nahmen der Gleichstellung ansto\ss en sollen. Der vorliegende Text wirft einen vergleichenden Blick auf die einschl\"agigen Vertragsbestimmungen und Richtlinien: inwiefern sie konvergieren und wo sie unterschiedliche Schutzniveaus und damit auch Hierarchien etablieren. Dabei gilt es, die Spezifika der einzelnen Diskriminierungsgr\"unde herauszuarbeiten, sie aber auch in ihrem Zusammenwirken in den Blick zu nehmen (,,Mehrfachdiskriminierung``). Abschlie\ss end werden die Vorgaben f\"ur Institutionen und Verfahren dargelegt.},
  isbn = {978-3-658-10976-9},
  keywords = {Diskriminierungstatbestände,EU-Grundrechtecharta,Europäische Union,Mehrfachdiskriminierung,Schutzniveaus},
  language = {de},
  series = {{Springer Reference Sozialwissenschaften}}
}

@inproceedings{kasyFairnessEqualityPower2021,
  title = {Fairness, {{Equality}}, and {{Power}} in {{Algorithmic Decision}}-{{Making}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Kasy, Maximilian and Abebe, Rediet},
  year = {2021},
  month = mar,
  pages = {576--586},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3442188.3445919},
  abstract = {Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same "merit." Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by "merit;" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.},
  file = {/home/sscher/Zotero/storage/T8Q47NAK/Kasy and Abebe - 2021 - Fairness, Equality, and Power in Algorithmic Decis.pdf},
  isbn = {978-1-4503-8309-7},
  keywords = {Algorithmic fairness,auditing,empirical economics,inequality,power},
  series = {{{FAccT}} '21}
}

@article{lopezReinforcingIntersectionalInequality2019,
  title = {Reinforcing {{Intersectional Inequality}} via the {{AMS Algorithm}} in {{Austria}}},
  author = {Lopez, Paola},
  year = {2019},
  pages = {21},
  abstract = {This paper examines the so-called AMS Algorithm from a mathematical perspective: this algorithmic system constitutes a predictive model that will be used by the Public Employment Service Austria (AMS) starting in 2020 to algorithmically classify job-seekers into three groups, each with different access to AMS support resources, according to their predicted chances on the labour market. Since the features gender, age, childcare responsibilities, disability and citizenship are explicitly implemented in the model and are thus linked to the availability of resources, this algorithmic system is to be considered very problematic. This paper is part of an ongoing research project, and it identifies three conceptual building blocks of the AMS Algorithm that are all based on human decisions and in which obvious societal bias can be located. Furthermore, this model is used as an illustrative example to address the larger question of what can be expected when predictions are made that are based solely on data that describes the past: If the predictions by these models result in unquestioned and confirmatory measures such as the redistribution of resources, a reproduction and reinforcement of inequality is possible. If these measures are now applied to vulnerable and highly dependent target groups, such as job-seekers, it will be more drastic: In a first step, these predictive models depict the reality of discrimination, then, in a second step, normatively reinforce it as a supposedly objective fact and finally, in a third step, return it to the social sphere by means of the resulting measures.},
  file = {/home/sscher/Zotero/storage/GERKMISS/Lopez - 2019 - Reinforcing Intersectional Inequality via the AMS .pdf},
  language = {en}
}


